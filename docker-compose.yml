services:
  backend:
    image: vllm/vllm-openai:v0.6.2
    container_name: vllm-backend
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    ports:
      - "8000:8000"
    ipc: host
    command:
      - "--model=meta-llama/Llama-3.1-8B-Instruct"
      - "--dtype=half"
      - "--tensor_parallel_size=4"
      - "--trust-remote-code"
      - "--max-model-len=8192"
    networks:
      - app-network

  api:
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: api-service
    depends_on:
      - backend
    ports:
      - "8001:8001"
    networks:
      - app-network
    volumes:
      - ./api/.chroma:/app/.chroma

  frontend:
    container_name: react-frontend
    build:
      context: ./frontend
      dockerfile: Dockerfile
    depends_on:
      - api
    ports:
      - "8080:80"
    networks:
      - app-network

networks:
  app-network:
    driver: bridge
