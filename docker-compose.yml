services:
  vllm-api:
    image: vllm/vllm-openai:v0.6.4.post1
    container_name: vllm-api
    runtime: nvidia
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - NVIDIA_VISIBLE_DEVICES=all
    ports:
      - "8000:8000"
    ipc: host
    command:
      - "--model=meta-llama/Llama-3.2-3B-Instruct"
      - "--trust-remote-code"
      - "--max-model-len=8192"
    networks:
      - app-network

  backend:
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: fastapi-backend
    depends_on:
      - vllm-api
    ports:
      - "8001:8001"
    networks:
      - app-network
    volumes:
      - ./api/.chroma:/app/.chroma

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: nextjs-frontend
    depends_on:
      - backend
    environment:
      - PORT=3000
    ports:
      - "3000:3000"
    networks:
      - app-network

  nginx:
    image: nginx:stable-alpine3.20
    container_name: nginx-proxy
    depends_on:
      - frontend
      - backend
    ports:
      - "8080:80"
    volumes:
      - ./frontend/nginx.conf:/etc/nginx/conf.d/default.conf
    networks:
      - app-network

networks:
  app-network:
    driver: bridge
