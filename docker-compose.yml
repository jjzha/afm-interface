services:
  backend:
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: fastapi-backend
    runtime: nvidia
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - NVIDIA_VISIBLE_DEVICES=0
    ports:
      - "8001:8001"
    networks:
      - app-network
    volumes:
      - /api/.biyaldi:/api/.biyaldi

  vllm-api:
    image: vllm/vllm-openai:v0.6.4.post1
    container_name: vllm-api
    depends_on:
      - backend
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=1
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    ports:
      - "8000:8000"
    ipc: host
    command:
      - "--model=Qwen/Qwen2-VL-2B-Instruct"
      # - "--tensor_parallel_size=1"
      - "--trust-remote-code"
    networks:
      - app-network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: nextjs-frontend
    depends_on:
      - backend
    environment:
      - PORT=3000
    ports:
      - "3000:3000"
    networks:
      - app-network

  nginx:
    image: nginx:stable-alpine3.20
    container_name: nginx-proxy
    depends_on:
      - frontend
      - backend
    ports:
      - "8080:80"
    volumes:
      - ./frontend/nginx.conf:/etc/nginx/conf.d/default.conf
    networks:
      - app-network

networks:
  app-network:
    driver: bridge
